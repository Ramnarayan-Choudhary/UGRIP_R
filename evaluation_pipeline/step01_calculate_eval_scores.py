'''
Step01_calculate_eval_scores.py

UGRIP Linguistics Olympiad Project
Updated 06/06/2024 @15:50 PM by Huanying (Joy) Yeh

Content:
- Take in the .json files generated by LLMs
- Pre-process the data to match the PuzzLing evaluation scripts format
- Saves outputs to a dedicated evaluations folder ("res" adjacent to "ref")

Notes: 
- Link to evaluation scripts: https://ukplab.github.io/PuzzLing-Machines/
'''

# TODO: this is a stub. We'll implement later if we need automated benchmarks.


# Currently, it takes in fake input scores
import random
import re
import os

# Original string with scores
original_string = """
EF_BLEU_SCORE: 82.73
EF_CHRF_SCORE: 97.09
EF_CTER_SCORE: 90.87
EF_EM_SCORE: 50.00
FE_BLEU_SCORE: 100.00
FE_CHRF_SCORE: 100.00
FE_CTER_SCORE: 100.00
FE_EM_SCORE: 100.00
BLEU_SCORE: 90.13
CHRF_SCORE: 98.34
CTER_SCORE: 94.78
EM_SCORE: 71.43
""".strip()

target_dir = 'My_PuzzLing_Test_Bench/scores'

# Ensure the target directory exists
os.makedirs(target_dir, exist_ok=True)

# Base filename format
filename_format = "{:04d}_lang{:02d}_scores.txt"

# Loop 10 times to create 10 files with random scores
for i in range(10):
    # Generate random numbers between 50 and 100
    random_numbers = [round(random.uniform(50.00, 100.00), 2) for _ in range(12)]
    result_string = re.sub(r"\d+\.\d+", lambda match: str(random_numbers.pop(0)), original_string)

    # Write to file
    filename = filename_format.format(i + 1, i + 1)
    with open(os.path.join(target_dir, filename), 'w') as file:
        file.write(result_string)

    print(f"Created dummy eval file: {filename}")


