{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_l: dyirbal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "############ \n",
    "# as of now this one is the final code without previous content \n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import openai\n",
    "import json\n",
    "\n",
    "# Define a chat function using API\n",
    "def chat(prompt, model, client):\n",
    "    message_text = [{\"role\": \"system\", \"content\": \"\"}, {\"role\": \"user\", \"content\": prompt}]\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,  # model = \"deployment_name\"\n",
    "        messages=message_text,\n",
    "        temperature=0,\n",
    "        max_tokens=200,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "    return completion\n",
    "\n",
    "# Data loader function\n",
    "def load_json_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def save_json_to_file(data, file_path):\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "# def load_problems_and_solutions(problems_dir, solutions_dir):\n",
    "#     problem_solution_pairs = []\n",
    "\n",
    "#     for filename in os.listdir(problems_dir):\n",
    "#         if filename.endswith('.json'):\n",
    "#             problem_file_path = os.path.join(problems_dir, filename)\n",
    "#             solution_file_path = os.path.join(solutions_dir, filename)\n",
    "            \n",
    "#             problem_data = load_json_from_file(problem_file_path)\n",
    "#             solution_data = load_json_from_file(solution_file_path)\n",
    "            \n",
    "#             source_language = problem_data.get(\"source_language\")\n",
    "#             target_language = problem_data.get(\"target_language\")\n",
    "#             train_data = problem_data.get(\"train\", [])\n",
    "#             test_problems = problem_data.get(\"test\", [])\n",
    "#             test_solutions = solution_data.get(\"test\", [])\n",
    "\n",
    "#             min_len = min(len(test_problems), len(test_solutions))\n",
    "#             if min_len > 0:\n",
    "#                 problem_solution_pairs.append({\n",
    "#                     \"source_language\": source_language,\n",
    "#                     \"target_language\": target_language,\n",
    "#                     \"train\": train_data,\n",
    "#                     \"test_problems\": [' '.join(filter(None, tp)) for tp in test_problems[:min_len]],  # Concatenate non-empty elements\n",
    "#                     \"test_solutions\": [' '.join(filter(None, ts)) for ts in test_solutions[:min_len]] \n",
    "#                 }\n",
    "#                 ) # Concatenate non-empty elements\n",
    "#                 # print(\"t_p:\",test_problems)\n",
    "#                 # print(\">>>>>>>>>>>>\")\n",
    "#                 # print(\"^^^^:\",test_solutions)\n",
    "                \n",
    "\n",
    "#     return problem_solution_pairs\n",
    "        \n",
    "# def load_problems_and_solutions(problems_dir, solutions_dir):\n",
    "#     problem_solution_pairs = []\n",
    "\n",
    "#     for filename in os.listdir(problems_dir):\n",
    "#         if filename.endswith('.json'):\n",
    "#             problem_file_path = os.path.join(problems_dir, filename)\n",
    "#             solution_file_path = os.path.join(solutions_dir, filename)\n",
    "            \n",
    "#             problem_data = load_json_from_file(problem_file_path)\n",
    "#             solution_data = load_json_from_file(solution_file_path)\n",
    "            \n",
    "#             source_language = problem_data.get(\"source_language\")\n",
    "#             target_language = problem_data.get(\"target_language\")\n",
    "#             train_data = problem_data.get(\"train\", [])\n",
    "#             test_problems = problem_data.get(\"test\", [])\n",
    "#             test_solutions = solution_data.get(\"test\", [])\n",
    "\n",
    "#             min_len = min(len(test_problems), len(test_solutions))\n",
    "#             if min_len > 0:\n",
    "#                 problem_solution_pairs.append({\n",
    "#                     \"source_language\": source_language,\n",
    "                    \n",
    "                    \n",
    "#                     \"target_language\": target_language,\n",
    "                    \n",
    "#                     \"train\": train_data,\n",
    "#                     \"test_problems\": [' '.join(filter(None, tp[:-1])) for tp in test_problems[:min_len]],  # Concatenate non-empty elements excluding the last element (\">\" or \"<\")\n",
    "#                     \"test_solutions\": [ts[1] for ts in test_solutions[:min_len] if len(ts) > 1]  # Extract only the solutions\n",
    "#                 })\n",
    "#             # print(\"t_p:\",test_problems)\n",
    "#             # print(\">>>>>>>>>>>>\")\n",
    "#             # print(\"^^^^:\",test_solutions)\n",
    "#             # print(\"s_l:\",source_language)\n",
    "#             # print(\"o_l:\",target_language)\n",
    "\n",
    "#     return problem_solution_pairs\n",
    "        \n",
    "def load_problems_and_solutions(problems_dir, solutions_dir):\n",
    "    problem_solution_pairs = []\n",
    "\n",
    "    for filename in os.listdir(problems_dir):\n",
    "        if filename.endswith('.json'):\n",
    "            problem_file_path = os.path.join(problems_dir, filename)\n",
    "            solution_file_path = os.path.join(solutions_dir, filename)\n",
    "            \n",
    "            problem_data = load_json_from_file(problem_file_path)\n",
    "            solution_data = load_json_from_file(solution_file_path)\n",
    "            \n",
    "            source_language = problem_data.get(\"source_language\")\n",
    "            target_language = problem_data.get(\"target_language\")\n",
    "            train_data = problem_data.get(\"train\", [])\n",
    "            test_problems = problem_data.get(\"test\", [])\n",
    "            test_solutions = solution_data.get(\"test\", [])\n",
    "\n",
    "            min_len = min(len(test_problems), len(test_solutions))\n",
    "            if min_len > 0:\n",
    "                problem_solution_pairs.append({\n",
    "                    \"source_language\": source_language,\n",
    "                    \"target_language\": target_language,\n",
    "                    \"train\": train_data,\n",
    "                    \"test_problems\": [' '.join(filter(None, tp)) for tp in test_problems[:min_len]],  # Concatenate non-empty elements\n",
    "                    \"test_solutions\": [' '.join(filter(None, ts)) for ts in test_solutions[:min_len]] \n",
    "                }\n",
    "                ) # Concatenate non-empty elements\n",
    "                # print(\"t_p:\",test_problems)\n",
    "                # print(\">>>>>>>>>>>>\")\n",
    "                # print(\"^^^^:\",test_solutions)\n",
    "                \n",
    "\n",
    "    return problem_solution_pairs\n",
    "\n",
    "# Interactive GPT function\n",
    "def iterative_gpt_interaction(train_data, test_problems, actual_solutions, gpt4_iterations, gpt35_iterations, client):\n",
    "    train_examples = \"\\n\".join([f\"Source: {example[0]}\\nTranslation: {example[1]}\" for example in train_data])\n",
    "    # in the above train examples i want to give all the trainig example as a train \n",
    "    # so in the above i used source and translation of each example which are mentioned in the particular quastion \n",
    "\n",
    "    attempted_solutions = []\n",
    "    all_hints = []\n",
    "    for test_problem, actual_solution in zip(test_problems, actual_solutions):\n",
    "        initial_prompt = (f\"Use the following training examples to understand the language translation:\\n\\n\"\n",
    "                          f\"{train_examples}\\n\\nNow, translate the following sentence:\\n\\n{test_problem}\\n\\n\"\n",
    "                          \"Provide the translation in a single sentence without extra information, symbols, or explanations.\")\n",
    "\n",
    "        response = chat(initial_prompt, \"gpt35turbo\", client)\n",
    "        response_content = response.choices[0].message.content\n",
    "\n",
    "        initial_prompt_for_gpt4 = (f\"Here we are providing the solution for each problem, so that you can use these hints \"\n",
    "                                   f\"to generate an optimized solution from gpt35turbo. Please use the solution part and generate \"\n",
    "                                   f\"hints for the following problem:\\n\\nProblem: {test_problem}\\n\\n\"\n",
    "                                   f\"Generated Solution: {response_content}\\n\\n\"\n",
    "                                   f\"Actual Solution: {actual_solution}\\n\\nHints:\")\n",
    "\n",
    "#         initial_prompt_for_gpt4 = \"\"\"Problem: {test_problem}\n",
    "\n",
    "# Expected solution: {actual_solution}\n",
    "\n",
    "# Student's solution: {response_content}\n",
    "\n",
    "# You are a helpful teaching assistant. Look at the linguistics olympiad problem above, its expected solution, and a student's attempted solution.\n",
    "# If the student's solution \n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "        hints = chat(initial_prompt_for_gpt4, \"gpt4\", client)\n",
    "        hints_content = hints.choices[0].message.content\n",
    "        all_hints.append(f\"The below all hints for this problem: {test_problem}\")\n",
    "        all_hints.append(f\"Initial Hints: {hints_content}\")\n",
    "\n",
    "        for i in range(gpt4_iterations):\n",
    "\n",
    "            # so for this we need to give a intail prompt to gpt35 with generated hints from 4 , and the intial prompt will be the  there with hints \n",
    "            # and this will  contains the problem in this prompts\n",
    "            intial_prompt=(f\"so please here we are providing the problem {test_problem}\\n, so please read this carefully and provide translation for this problem \\n. \"\n",
    "                           f\" so for the above problem you can use this hints {hints_content}\\n for the translation.\"\n",
    "                           f\"Refine the translation in a single sentence without extra information, symbols, or explanations.\"\n",
    "                           f\" please dont add extra words and symbols  in the final response which you are generating\"\n",
    "                           ) \n",
    "\n",
    "            \n",
    "            refinement_prompt = (f\"{response_content}\\n\\nHints:\\n{hints_content}\\n\\n\"\n",
    "                                 \"Refine the translation in a single sentence without extra information, symbols, or explanations.\")\n",
    "            refined_response = chat(intial_prompt, \"gpt35turbo\", client)\n",
    "            response_content = refined_response.choices[0].message.content\n",
    "\n",
    "            # difference_analysis_prompt = (f\"Actual Solution: {actual_solution}\\nGenerated Solution: {response_content}\\n\\n\"\n",
    "            #                               \"Generate hints for the differences and how to improve the solution:\"\n",
    "            #                               f\"help in identify the problem occures in {response_content} and also analyze where the error is occuring \")\n",
    "            \n",
    "            difference_analysis_prompt = f\"\"\"Actual Solution: {actual_solution}\n",
    "            Revised Generated Solution: {response_content}\\n\\n\n",
    "Given above is the revised solution generated by gpt35turbo. Compare its solution with the actual solution to identify where and why do error(s) occur. \n",
    "Accordingly, generate a hint that helps gpt35turbo improve its solution. Directly address gpt35turbo as if it is a student and you are a teacher. Though you are premitted to point out mistakes in the generated solution, make sure to not reveal any details of the actual solution in your hint. Also note that gpt35turbo does not have access to the actual solution.\n",
    "\"\"\"\n",
    "\n",
    "            hints = chat(difference_analysis_prompt, \"gpt4\", client)\n",
    "            hints_content = hints.choices[0].message.content\n",
    "            all_hints.append(f\"Hint Iteration {i+1}: {hints_content}\")\n",
    "\n",
    "        all_hints.append(\"--------------------------------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "        attempted_solutions.append(response_content)\n",
    "\n",
    "    return attempted_solutions, all_hints\n",
    "\n",
    "def main():\n",
    "    problems_dir = \"/Users/ramnarayanchoudhary/Desktop/UGRIP/new_repo/ugrip24-ling/global_iol_analysis/BBB_all_puzzling_problems/data_public_data_dev\"  # Update with the correct path\n",
    "    solutions_dir = \"/Users/ramnarayanchoudhary/Desktop/UGRIP/new_repo/ugrip24-ling/global_iol_analysis/BBB_all_puzzling_problems/data_public_reference_data_dev\"  # Update with the correct path  \n",
    "    output_dir = \"output_directory_of_gpt_interactions\"  # Update with the desired output directory\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    problem_solution_pairs = load_problems_and_solutions(problems_dir, solutions_dir)\n",
    "\n",
    "    # Initialize the OpenAI client\n",
    "    api_key = \"037155e1b16a432fa836637370eca0e3\"  # Replace with your actual API key\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=\"https://cullmsouthindia.openai.azure.com/\",\n",
    "        api_key=api_key,\n",
    "        api_version=\"2024-02-15-preview\"\n",
    "    )\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for pair in problem_solution_pairs:\n",
    "        source_language = pair[\"source_language\"]\n",
    "        target_language = pair[\"target_language\"]\n",
    "        train_data = pair[\"train\"]\n",
    "        test_problems = pair[\"test_problems\"]\n",
    "        test_solutions = pair[\"test_solutions\"]\n",
    "\n",
    "        final_responses = None  # Initialize final_responses to ensure it's defined\n",
    "        all_hints = None  # Initialize hints to ensure it's defined\n",
    "\n",
    "        if source_language==\"dyirbal\":\n",
    "         final_responses, all_hints = iterative_gpt_interaction(train_data, test_problems, test_solutions, gpt4_iterations=1, gpt35_iterations=1, client=client)\n",
    "\n",
    "        #final_responses, all_hints = iterative_gpt_interaction(train_data, test_problems, test_solutions, gpt4_iterations=1, gpt35_iterations=1, client=client)\n",
    "\n",
    "        if final_responses:  # Only add to results if final_responses is set\n",
    "            language_pair = f\"{source_language}-{target_language}\"\n",
    "            print(\"s_l:\",source_language)\n",
    "            if language_pair not in results:\n",
    "                results[language_pair] = []\n",
    "            results[language_pair].append({\n",
    "               # \"train\": train_data,\n",
    "                \"test_problems\": test_problems,\n",
    "                \"test_solutions\": test_solutions,\n",
    "                \"attempted_solutions\": final_responses,\n",
    "                \"hints\": all_hints\n",
    "            })\n",
    "\n",
    "    # Save results in language-specific files\n",
    "    for language_pair, data in results.items():\n",
    "        output_file_path = os.path.join(output_dir, f\"{language_pair}.json\")\n",
    "        save_json_to_file(data, output_file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"'$.messages[1].content' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 159\u001b[0m\n\u001b[1;32m    156\u001b[0m         save_json_to_file(data, output_file_path)\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 159\u001b[0m     main()\n\u001b[1;32m    160\u001b[0m \u001b[39m# import os\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m# from openai import AzureOpenAI\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m# import openai\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# if __name__ == \"__main__\":\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39m#     main()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 139\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m all_hints \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# Initialize hints to ensure it's defined\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[39mif\u001b[39;00m source_language\u001b[39m==\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meuskara\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m  final_responses, all_hints \u001b[39m=\u001b[39m iterative_gpt_interaction(train_data, test_problems, test_solutions, gpt4_iterations\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, gpt35_iterations\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, client\u001b[39m=\u001b[39;49mclient)\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m final_responses:  \u001b[39m# Only add to results if final_responses is set\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     language_pair \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00msource_language\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00mtarget_language\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[17], line 93\u001b[0m, in \u001b[0;36miterative_gpt_interaction\u001b[0;34m(train_data, test_problems, actual_solutions, gpt4_iterations, gpt35_iterations, client)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(gpt4_iterations):\n\u001b[1;32m     88\u001b[0m     message_text \u001b[39m=\u001b[39m [\n\u001b[1;32m     89\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: prev_response},\n\u001b[1;32m     90\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse_content\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mHints:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mhints_content\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m}\n\u001b[1;32m     91\u001b[0m     ]\n\u001b[0;32m---> 93\u001b[0m     refined_response \u001b[39m=\u001b[39m chat(message_text, \u001b[39m\"\u001b[39;49m\u001b[39mgpt35turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m, client)\n\u001b[1;32m     94\u001b[0m     response_content \u001b[39m=\u001b[39m refined_response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mcontent\n\u001b[1;32m     96\u001b[0m     difference_analysis_prompt \u001b[39m=\u001b[39m (\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mActual Solution: \u001b[39m\u001b[39m{\u001b[39;00mactual_solution\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mGenerated Solution: \u001b[39m\u001b[39m{\u001b[39;00mresponse_content\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m                                   \u001b[39m\"\u001b[39m\u001b[39mGenerate hints for the differences and how to improve the solution:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m                                   \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhelp in identify the problem occures in \u001b[39m\u001b[39m{\u001b[39;00mresponse_content\u001b[39m}\u001b[39;00m\u001b[39m and also analyze where the error is occuring \u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m, in \u001b[0;36mchat\u001b[0;34m(prompt, model, client)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mchat\u001b[39m(prompt, model, client):\n\u001b[1;32m      8\u001b[0m     message_text \u001b[39m=\u001b[39m [{\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m}, {\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: prompt}]\n\u001b[0;32m----> 9\u001b[0m     completion \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     10\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,  \u001b[39m# model = \"deployment_name\"\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m         messages\u001b[39m=\u001b[39;49mmessage_text,\n\u001b[1;32m     12\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m         max_tokens\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m,\n\u001b[1;32m     14\u001b[0m         top_p\u001b[39m=\u001b[39;49m\u001b[39m0.95\u001b[39;49m,\n\u001b[1;32m     15\u001b[0m         frequency_penalty\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     16\u001b[0m         presence_penalty\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     17\u001b[0m         stop\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_utils/_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 277\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py:606\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    574\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    575\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    605\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 606\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    607\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    608\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    609\u001b[0m             {\n\u001b[1;32m    610\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    611\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    612\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    613\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    614\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    615\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    616\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[1;32m    617\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    618\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    619\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mparallel_tool_calls\u001b[39;49m\u001b[39m\"\u001b[39;49m: parallel_tool_calls,\n\u001b[1;32m    620\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    621\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    622\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    623\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    624\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    625\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream_options\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream_options,\n\u001b[1;32m    626\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    627\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    628\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    629\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[1;32m    630\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    631\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    632\u001b[0m             },\n\u001b[1;32m    633\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    634\u001b[0m         ),\n\u001b[1;32m    635\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    636\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    637\u001b[0m         ),\n\u001b[1;32m    638\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    639\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    640\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    641\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_base_client.py:1240\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1227\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1228\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1235\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1236\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1237\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1238\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1239\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    913\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    920\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 921\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    922\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    923\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    924\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    925\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    926\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    927\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_base_client.py:1020\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[1;32m   1019\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mRe-raising status error\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1020\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1022\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[1;32m   1023\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m   1024\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1027\u001b[0m     stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m   1028\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"'$.messages[1].content' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import openai\n",
    "import json\n",
    "\n",
    "# Define a chat function using API\n",
    "def chat(prompt, model, client):\n",
    "    message_text = [{\"role\": \"system\", \"content\": \"\"}, {\"role\": \"user\", \"content\": prompt}]\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,  # model = \"deployment_name\"\n",
    "        messages=message_text,\n",
    "        temperature=0,\n",
    "        max_tokens=200,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "    return completion\n",
    "\n",
    "# Data loader function\n",
    "def load_json_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def save_json_to_file(data, file_path):\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def load_problems_and_solutions(problems_dir, solutions_dir):\n",
    "    problem_solution_pairs = []\n",
    "\n",
    "    for filename in os.listdir(problems_dir):\n",
    "        if filename.endswith('.json'):\n",
    "            problem_file_path = os.path.join(problems_dir, filename)\n",
    "            solution_file_path = os.path.join(solutions_dir, filename)\n",
    "            \n",
    "            problem_data = load_json_from_file(problem_file_path)\n",
    "            solution_data = load_json_from_file(solution_file_path)\n",
    "            \n",
    "            source_language = problem_data.get(\"source_language\")\n",
    "            target_language = problem_data.get(\"target_language\")\n",
    "            train_data = problem_data.get(\"train\", [])\n",
    "            test_problems = problem_data.get(\"test\", [])\n",
    "            test_solutions = solution_data.get(\"test\", [])\n",
    "\n",
    "            min_len = min(len(test_problems), len(test_solutions))\n",
    "            if min_len > 0:\n",
    "                problem_solution_pairs.append({\n",
    "                    \"source_language\": source_language,\n",
    "                    \"target_language\": target_language,\n",
    "                    \"train\": train_data,\n",
    "                    \"test_problems\": [' '.join(filter(None, tp[:-1])) for tp in test_problems[:min_len]],  # Concatenate non-empty elements excluding the last element (\">\" or \"<\")\n",
    "                    \"test_solutions\": [ts[1] for ts in test_solutions[:min_len] if len(ts) > 1]  # Extract only the solutions\n",
    "                })\n",
    "\n",
    "    return problem_solution_pairs\n",
    "\n",
    "# Interactive GPT function\n",
    "def iterative_gpt_interaction(train_data, test_problems, actual_solutions, gpt4_iterations, gpt35_iterations, client):\n",
    "    train_examples = \"\\n\".join([f\"Source: {example[0]}\\nTranslation: {example[1]}\" for example in train_data])\n",
    "\n",
    "    attempted_solutions = []\n",
    "    all_hints = []\n",
    "    prev_response = \"\"  # To store the previous response\n",
    "\n",
    "    for test_problem, actual_solution in zip(test_problems, actual_solutions):\n",
    "        initial_prompt = (f\"Use the following training examples to understand the language translation:\\n\\n\"\n",
    "                          f\"{train_examples}\\n\\nNow, translate the following sentence:\\n\\n{test_problem}\\n\\n\"\n",
    "                          \"Provide the translation in a single sentence without extra information, symbols, or explanations.\")\n",
    "\n",
    "        response = chat(initial_prompt, \"gpt35turbo\", client)\n",
    "        response_content = response.choices[0].message.content\n",
    "\n",
    "        initial_prompt_for_gpt4 = (f\"Here we are providing the solution for each problem, so that you can use these hints \"\n",
    "                                   f\"to generate an optimized solution from gpt35turbo. Please use the solution part and generate \"\n",
    "                                   f\"hints for the following problem:\\n\\nProblem: {test_problem}\\n\\n\"\n",
    "                                   f\"Generated Solution: {response_content}\\n\\n\"\n",
    "                                   f\"Actual Solution: {actual_solution}\\n\\nHints:\")\n",
    "\n",
    "        hints = chat(initial_prompt_for_gpt4, \"gpt4\", client)\n",
    "        hints_content = hints.choices[0].message.content\n",
    "        all_hints.append(f\"The below all hints for this problem: {test_problem}\")\n",
    "        all_hints.append(f\"Initial Hints: {hints_content}\")\n",
    "\n",
    "        for i in range(gpt4_iterations):\n",
    "            message_text = [\n",
    "                {\"role\": \"system\", \"content\": prev_response},\n",
    "                {\"role\": \"user\", \"content\": f\"{response_content}\\n\\nHints:\\n{hints_content}\"}\n",
    "            ]\n",
    "\n",
    "            refined_response = chat(message_text, \"gpt35turbo\", client)\n",
    "            response_content = refined_response.choices[0].message.content\n",
    "\n",
    "            difference_analysis_prompt = (f\"Actual Solution: {actual_solution}\\nGenerated Solution: {response_content}\\n\\n\"\n",
    "                                          \"Generate hints for the differences and how to improve the solution:\"\n",
    "                                          f\"help in identify the problem occures in {response_content} and also analyze where the error is occuring \")\n",
    "            hints = chat(difference_analysis_prompt, \"gpt4\", client)\n",
    "            hints_content = hints.choices[0].message.content\n",
    "            all_hints.append(f\"Hint Iteration {i+1}: {hints_content}\")\n",
    "\n",
    "            prev_response = response_content\n",
    "\n",
    "        all_hints.append(\"--------------------------------------------------------------------------------\\n\")\n",
    "        attempted_solutions.append(response_content)\n",
    "\n",
    "    return attempted_solutions, all_hints\n",
    "\n",
    "def main():\n",
    "    problems_dir = \"/Users/ramnarayanchoudhary/Desktop/UGRIP/new_repo/ugrip24-ling/global_iol_analysis/BBB_all_puzzling_problems/data_public_data_dev\"  # Update with the correct path\n",
    "    solutions_dir = \"/Users/ramnarayanchoudhary/Desktop/UGRIP/new_repo/ugrip24-ling/global_iol_analysis/BBB_all_puzzling_problems/data_public_reference_data_dev\"  # Update with the correct path  \n",
    "    output_dir = \"output_directory_of_gpt_interactions\"  # Update with the desired output directory\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    problem_solution_pairs = load_problems_and_solutions(problems_dir, solutions_dir)\n",
    "\n",
    "    # Initialize the OpenAI client\n",
    "    api_key = \"037155e1b16a432fa836637370eca0e3\"  # Replace with your actual API key\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=\"https://cullmsouthindia.openai.azure.com/\",\n",
    "        api_key=api_key,\n",
    "        api_version=\"2024-02-15-preview\"\n",
    "    )\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for pair in problem_solution_pairs:\n",
    "        source_language = pair[\"source_language\"]\n",
    "        target_language = pair[\"target_language\"]\n",
    "        train_data = pair[\"train\"]\n",
    "        test_problems = pair[\"test_problems\"]\n",
    "        test_solutions = pair[\"test_solutions\"]\n",
    "\n",
    "        final_responses = None  # Initialize final_responses to ensure it's defined\n",
    "        all_hints = None  # Initialize hints to ensure it's defined\n",
    "        if source_language==\"euskara\":\n",
    "         final_responses, all_hints = iterative_gpt_interaction(train_data, test_problems, test_solutions, gpt4_iterations=1, gpt35_iterations=1, client=client)\n",
    "\n",
    "        if final_responses:  # Only add to results if final_responses is set\n",
    "            language_pair = f\"{source_language}-{target_language}\"\n",
    "            print(\"s_l:\",source_language)\n",
    "            if language_pair not in results:\n",
    "                results[language_pair] = []\n",
    "            results[language_pair].append({\n",
    "                \"test_problems\": test_problems,\n",
    "                \"test_solutions\": test_solutions,\n",
    "                \"attempted_solutions\": final_responses,\n",
    "                \"hints\": all_hints\n",
    "            })\n",
    "\n",
    "    # Save results in language-specific files\n",
    "    for language_pair, data in results.items():\n",
    "        output_file_path = os.path.join(output_dir, f\"{language_pair}.json\")\n",
    "        save_json_to_file(data, output_file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "# import os\n",
    "# from openai import AzureOpenAI\n",
    "# import openai\n",
    "# import json\n",
    "\n",
    "# # Define a chat function using API\n",
    "# def chat(message_text, model, client):\n",
    "#     completion = client.chat.completions.create(\n",
    "#         model=model,  # model = \"deployment_name\"\n",
    "#         messages=message_text,\n",
    "#         temperature=0,\n",
    "#         max_tokens=200,\n",
    "#         top_p=0.95,\n",
    "#         frequency_penalty=0,\n",
    "#         presence_penalty=0,\n",
    "#         stop=None\n",
    "#     )\n",
    "#     return completion\n",
    "\n",
    "# # Data loader function\n",
    "# def load_json_from_file(file_path):\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         return json.load(file)\n",
    "\n",
    "# def save_json_to_file(data, file_path):\n",
    "#     os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "#     with open(file_path, 'w') as file:\n",
    "#         json.dump(data, file, indent=4)\n",
    "\n",
    "# def load_problems_and_solutions(problems_dir, solutions_dir):\n",
    "#     problem_solution_pairs = []\n",
    "\n",
    "#     for filename in os.listdir(problems_dir):\n",
    "#         if filename.endswith('.json'):\n",
    "#             problem_file_path = os.path.join(problems_dir, filename)\n",
    "#             solution_file_path = os.path.join(solutions_dir, filename)\n",
    "            \n",
    "#             problem_data = load_json_from_file(problem_file_path)\n",
    "#             solution_data = load_json_from_file(solution_file_path)\n",
    "            \n",
    "#             source_language = problem_data.get(\"source_language\")\n",
    "#             target_language = problem_data.get(\"target_language\")\n",
    "#             train_data = problem_data.get(\"train\", [])\n",
    "#             test_problems = problem_data.get(\"test\", [])\n",
    "#             test_solutions = solution_data.get(\"test\", [])\n",
    "\n",
    "#             min_len = min(len(test_problems), len(test_solutions))\n",
    "#             if min_len > 0:\n",
    "#                 problem_solution_pairs.append({\n",
    "#                     \"source_language\": source_language,\n",
    "#                     \"target_language\": target_language,\n",
    "#                     \"train\": train_data,\n",
    "#                     \"test_problems\": [' '.join(filter(None, tp[:-1])) for tp in test_problems[:min_len]],  # Concatenate non-empty elements excluding the last element (\">\" or \"<\")\n",
    "#                     \"test_solutions\": [ts[1] for ts in test_solutions[:min_len] if len(ts) > 1]  # Extract only the solutions\n",
    "#                 })\n",
    "\n",
    "#     return problem_solution_pairs\n",
    "\n",
    "# # Interactive GPT function\n",
    "# def iterative_gpt_interaction(train_data, test_problems, actual_solutions, gpt4_iterations, gpt35_iterations, client):\n",
    "#     train_examples = \"\\n\".join([f\"Source: {example[0]}\\nTranslation: {example[1]}\" for example in train_data])\n",
    "\n",
    "#     attempted_solutions = []\n",
    "#     all_hints = []\n",
    "#     prev_response = \"\"  # To store the previous response\n",
    "\n",
    "#     for test_problem, actual_solution in zip(test_problems, actual_solutions):\n",
    "#         initial_prompt = (f\"Use the following training examples to understand the language translation:\\n\\n\"\n",
    "#                           f\"{train_examples}\\n\\nNow, translate the following sentence:\\n\\n{test_problem}\\n\\n\"\n",
    "#                           \"Provide the translation in a single sentence without extra information, symbols, or explanations.\")\n",
    "\n",
    "#         response = chat([{\"role\": \"system\", \"content\": prev_response}, {\"role\": \"user\", \"content\": initial_prompt}], \"gpt35turbo\", client)\n",
    "#         response_content = response.choices[0].message.content.strip()\n",
    "\n",
    "#         initial_prompt_for_gpt4 = (f\"Here we are providing the solution for each problem, so that you can use these hints \"\n",
    "#                                    f\"to generate an optimized solution from gpt35turbo. Please use the solution part and generate \"\n",
    "#                                    f\"hints for the following problem:\\n\\nProblem: {test_problem}\\n\\n\"\n",
    "#                                    f\"Generated Solution: {response_content}\\n\\n\"\n",
    "#                                    f\"Actual Solution: {actual_solution}\\n\\nHints:\")\n",
    "\n",
    "#         hints = chat([{\"role\": \"system\", \"content\": prev_response}, {\"role\": \"user\", \"content\": initial_prompt_for_gpt4}], \"gpt4\", client)\n",
    "#         hints_content = hints.choices[0].message.content.strip()\n",
    "#         all_hints.append(f\"The below all hints for this problem: {test_problem}\")\n",
    "#         all_hints.append(f\"Initial Hints: {hints_content}\")\n",
    "\n",
    "#         for i in range(gpt4_iterations):\n",
    "#             message_text = [\n",
    "#                 {\"role\": \"system\", \"content\": prev_response},\n",
    "#                 {\"role\": \"user\", \"content\": f\"{response_content}\\n\\nHints:\\n{hints_content}\"}\n",
    "#             ]\n",
    "\n",
    "#             refined_response = chat(message_text, \"gpt35turbo\", client)\n",
    "#             response_content = refined_response.choices[0].message.content.strip()\n",
    "\n",
    "#             difference_analysis_prompt = (f\"Actual Solution: {actual_solution}\\nGenerated Solution: {response_content}\\n\\n\"\n",
    "#                                           \"Generate hints for the differences and how to improve the solution:\"\n",
    "#                                           f\"help in identify the problem occures in {response_content} and also analyze where the error is occuring \")\n",
    "#             hints = chat([{\"role\": \"system\", \"content\": prev_response}, {\"role\": \"user\", \"content\": difference_analysis_prompt}], \"gpt4\", client)\n",
    "#             hints_content = hints.choices[0].message.content.strip()\n",
    "#             all_hints.append(f\"Hint Iteration {i+1}: {hints_content}\")\n",
    "\n",
    "#             prev_response = response_content\n",
    "\n",
    "#         all_hints.append(\"--------------------------------------------------------------------------------\\n\")\n",
    "#         attempted_solutions.append(response_content)\n",
    "\n",
    "#     return attempted_solutions, all_hints\n",
    "\n",
    "# def main():\n",
    "#     problems_dir = \"/Users/ramnarayanchoudhary/Desktop/UGRIP/new_repo/ugrip24-ling/global_iol_analysis/BBB_all_puzzling_problems/data_public_data_dev\"  # Update with the correct path\n",
    "#     solutions_dir = \"/Users/ramnarayanchoudhary/Desktop/UGRIP/new_repo/ugrip24-ling/global_iol_analysis/BBB_all_puzzling_problems/data_public_reference_data_dev\"  # Update with the correct path  \n",
    "#     output_dir = \"output_directory_of_gpt_interactions\"  # Update with the desired output directory\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "\n",
    "#     problem_solution_pairs = load_problems_and_solutions(problems_dir, solutions_dir)\n",
    "\n",
    "#     # Initialize the OpenAI client\n",
    "#     api_key = \"037155e1b16a432fa836637370eca0e3\"  # Replace with your actual API key\n",
    "#     client = AzureOpenAI(\n",
    "#         azure_endpoint=\"https://cullmsouthindia.openai.azure.com/\",\n",
    "#         api_key=api_key,\n",
    "#         api_version=\"2024-02-15-preview\"\n",
    "#     )\n",
    "\n",
    "#     results = {}\n",
    "\n",
    "#     for pair in problem_solution_pairs:\n",
    "#         source_language = pair[\"source_language\"]\n",
    "#         target_language = pair[\"target_language\"]\n",
    "#         train_data = pair[\"train\"]\n",
    "#         test_problems = pair[\"test_problems\"]\n",
    "#         test_solutions = pair[\"test_solutions\"]\n",
    "\n",
    "#         final_responses = None  # Initialize final_responses to ensure it's defined\n",
    "#         all_hints = None  # Initialize hints to ensure it's defined\n",
    "#         if source_language==\"euskara\":\n",
    "#          final_responses, all_hints = iterative_gpt_interaction(train_data, test_problems, test_solutions, gpt4_iterations=1, gpt35_iterations=1, client=client)\n",
    "\n",
    "#         if final_responses:  # Only add to results if final_responses is set\n",
    "#             language_pair = f\"{source_language}-{target_language}\"\n",
    "#             print(\"s_l:\",source_language)\n",
    "#             if language_pair not in results:\n",
    "#                 results[language_pair] = []\n",
    "#             results[language_pair].append({\n",
    "#                 \"test_problems\": test_problems,\n",
    "#                 \"test_solutions\": test_solutions,\n",
    "#                 \"attempted_solutions\": final_responses,\n",
    "#                 \"hints\": all_hints\n",
    "#             })\n",
    "\n",
    "#     # Save results in language-specific files\n",
    "#     for language_pair, data in results.items():\n",
    "#         output_file_path = os.path.join(output_dir, f\"{language_pair}.json\")\n",
    "#         save_json_to_file(data, output_file_path)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
