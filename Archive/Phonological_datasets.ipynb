{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def init_phonological_generalizations_data(directory=None, output_dir=None):\n",
    "    '''\n",
    "    This function initializes and categorizes phonological generalizations data from a given directory.\n",
    "    It returns a dictionary where keys are the problem types (morphology, transliteration, stress, multilingual) \n",
    "    and values are lists of corresponding problem datasets. It also saves each problem as a separate JSON file\n",
    "    with the source language and type included in the filename.\n",
    "    '''\n",
    "    url = 'https://github.com/saujasv/phonological-generalizations.git'\n",
    "    json_dir = 'phonological-generalizations/data/problems'\n",
    "\n",
    "    # Clone the repository if it does not exist\n",
    "    if not os.path.exists(json_dir):\n",
    "        os.system(f\"git clone {url}\")\n",
    "    \n",
    "    # Dictionary to hold categorized problems\n",
    "    categorized_problems = {\n",
    "        'morphology': [],\n",
    "        'transliteration': [],\n",
    "        'stress': [],\n",
    "        'multilingual': []\n",
    "    }\n",
    "\n",
    "    # Check if the directory exists\n",
    "    if os.path.exists(json_dir):\n",
    "        # Load all JSON files in the directory\n",
    "        for filename in os.listdir(json_dir):\n",
    "            if filename.endswith('.json'):\n",
    "                with open(os.path.join(json_dir, filename), 'r') as file:\n",
    "                    problem_data_list = json.load(file)\n",
    "\n",
    "                    # Ensure problem_data_list is a list\n",
    "                    if isinstance(problem_data_list, list):\n",
    "                        # Iterate over each problem data in the list\n",
    "                        for problem_data in problem_data_list:\n",
    "                            # Determine the type of the problem from the 'type' field\n",
    "                            problem_type = problem_data.get('type', None)\n",
    "                            if problem_type:\n",
    "                                # Extract source and target languages\n",
    "                                languages = problem_data.get('languages', [])\n",
    "                                source_language = languages[0] if len(languages) > 0 else \"\"\n",
    "                                target_language = languages[1] if len(languages) > 1 else \"\"\n",
    "\n",
    "                                # Extract meta information if available\n",
    "                                meta = problem_data.get('meta', \"\")\n",
    "\n",
    "                                # Extract other relevant information\n",
    "                                families = problem_data.get('families', [])\n",
    "                                columns = problem_data.get('columns', [])\n",
    "\n",
    "                                # Split the data into train and test sets\n",
    "                                train_data = []\n",
    "                                test_data = []\n",
    "                                for item in problem_data['data']:\n",
    "                                    if '?' in item[0] or '?' in item[1]:\n",
    "                                        test_data.append(item)\n",
    "                                    else:\n",
    "                                        train_data.append(item)\n",
    "                                \n",
    "                                # Create the new problem structure with train and test data\n",
    "                                new_problem_data = {\n",
    "                                    \"source_language\": source_language,\n",
    "                                    \"target_language\": target_language,\n",
    "                                    #\"meta\": meta,\n",
    "                                    \"type\": problem_type,\n",
    "                                    \"families\": families,\n",
    "                                    #\"columns\": columns,\n",
    "                                    \"train\": train_data,\n",
    "                                    \"test\": test_data\n",
    "                                }\n",
    "\n",
    "                                # Append the problem data to the corresponding category\n",
    "                                categorized_problems[problem_type].append(new_problem_data)\n",
    "\n",
    "                                # Save each problem as a JSON file using source language and type\n",
    "                                if output_dir:\n",
    "                                    category_dir = os.path.join(output_dir, problem_type)\n",
    "                                    if not os.path.exists(category_dir):\n",
    "                                        os.makedirs(category_dir)\n",
    "\n",
    "                                    filename = f\"{source_language}_{problem_type}.json\"\n",
    "                                    problem_file = os.path.join(category_dir, filename)\n",
    "\n",
    "                                    with open(problem_file, 'w') as pf:\n",
    "                                        json.dump(new_problem_data, pf, indent=4)\n",
    "\n",
    "    return categorized_problems\n",
    "\n",
    "# Usage example:\n",
    "output_directory = \"./categorized_problems1\"\n",
    "categorized_problems = init_phonological_generalizations_data(output_dir=output_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ugrip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
